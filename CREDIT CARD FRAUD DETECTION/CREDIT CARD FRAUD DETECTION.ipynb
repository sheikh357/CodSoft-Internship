{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02b3c27",
   "metadata": {},
   "source": [
    "# Build a machine learning model to identify fraudulent credit card\n",
    "transactions.\n",
    "Preprocess and normalize the transaction data, handle class\n",
    "imbalance issues, and split the dataset into training and testing sets.\n",
    "Train a classification algorithm, such as logistic regression or random\n",
    "forests, to classify transactions as fraudulent or genuine.\n",
    "Evaluate the model's performance using metrics like precision, recall,and F1-score, and consider techniques like oversampling or\n",
    "undersampling for improving results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f7d2f",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries and Load Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca9f48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Load your dataset ('creditcard.csv' in this example)\n",
    "# Replace 'creditcard.csv' with your actual dataset file\n",
    "dataset = pd.read_csv('creditcard.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b04509",
   "metadata": {},
   "source": [
    "# Step 2: Define Features and Target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc3b7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = dataset.iloc[:, 1:30].values  # Adjust columns as needed\n",
    "y = dataset.iloc[:, 30].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a73d3e",
   "metadata": {},
   "source": [
    "# Step 3: Split the Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8510b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2782b2f",
   "metadata": {},
   "source": [
    "# Step 4: Standardize Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28e26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a33da",
   "metadata": {},
   "source": [
    "# Step 5: Handle Class Imbalance (Oversampling and Undersampling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9061e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance using oversampling (you can adjust the strategy)\n",
    "ros = RandomOverSampler(sampling_strategy=0.5, random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = ros.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725422db",
   "metadata": {},
   "source": [
    "# Step6:Train a Logistic RegressionClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "918cc55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "classifier.fit(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5a2ca",
   "metadata": {},
   "source": [
    "# Step 7: Make Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9b4a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2107b33",
   "metadata": {},
   "source": [
    "# Step 8: Evaluate the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b87b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.988729328324146\n",
      "Confusion Matrix:\n",
      " [[56231   633]\n",
      " [    9    89]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56864\n",
      "           1       0.12      0.91      0.22        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.56      0.95      0.61     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951707ce",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection Conclusion:\n",
    "\n",
    "In this code, we developed a credit card fraud detection model using logistic regression. The goal of this project is to identify fraudulent credit card transactions from a dataset that contains both genuine and fraudulent transactions. Here are the main takeaways:\n",
    "\n",
    "1. Data Loading and Preparation:\n",
    "\n",
    "We imported necessary libraries and loaded the dataset ('creditcard.csv' in this example).\n",
    "Features (X) and the target variable (y) were defined.\n",
    "The dataset was split into training and testing sets (80% training, 20% testing) to evaluate the model.\n",
    "\n",
    "2. Data Standardization:\n",
    "\n",
    "We standardized the features to have a mean of 0 and a standard deviation of 1. Standardization helps in improving model performance.\n",
    "\n",
    "3. Handling Class Imbalance:\n",
    "\n",
    "To address the class imbalance problem (where fraudulent transactions are rare), we employed oversampling using the RandomOverSampler from the imblearn library. Oversampling creates synthetic examples of the minority class.\n",
    "\n",
    "4. Model Training:\n",
    "\n",
    "A logistic regression classifier was chosen for its simplicity and interpretability.\n",
    "The classifier was trained using the oversampled training data.\n",
    "\n",
    "5. Model Evaluation:\n",
    "\n",
    "We made predictions on the test set using the trained model.\n",
    "Model performance was evaluated using metrics such as accuracy, confusion matrix, and classification report.\n",
    "The classification report provides insights into precision, recall, and F1-score for both classes (fraudulent and genuine transactions).\n",
    "Overall, this code serves as a foundational framework for credit card fraud detection. Keep in mind that this is a basic implementation, and for real-world applications, further steps such as hyperparameter tuning, feature engineering, and more advanced modeling techniques may be necessary to achieve higher accuracy and reliability.\n",
    "\n",
    "The code can be further expanded and customized to suit the specific requirements and nuances of your dataset and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57d258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
